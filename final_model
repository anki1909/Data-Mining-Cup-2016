from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingClassifier
from sklearn import svm
from sklearn.linear_model import LogisticRegression

from sklearn import decomposition,pipeline
from collections import defaultdict
import numpy as np
import pandas as pd
from sklearn.cross_validation import StratifiedKFold
from sklearn import ensemble, preprocessing, grid_search, cross_validation
from sklearn import metrics 
from sklearn.calibration import CalibratedClassifierCV

import scipy.stats as scs
from sklearn.svm import SVC
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer

import scipy as sp


"""
print actual
print pred
 print np.abs(actual -pred)

"""


def cal_error(actual , pred):
    
    return np.sum(np.abs(actual -pred))


"""
Quantity ==0 ke prediction ko 0 dena h
"""
if __name__ == '__main__':
    
    train = pd.read_table('../input/orders_train.txt',sep = ';')#, parse_dates = 'orderDate')
    test = pd.read_table('../input/orders_class.txt',sep = ';')
    
    index = train.quantity >= train.returnQuantity
    train = train.loc[index] 
    
    submission_frame = test[['orderID','articleID','colorCode','sizeCode']].copy()
    
    
    l = train[['articleID','customerID','quantity']].append(test[['articleID','customerID','quantity']])
    l  = l[['articleID','customerID','quantity']].groupby(['articleID','customerID']).agg('sum').reset_index()
    l.columns = ['articleID','customerID','qntt']
    train = train.merge(l,on = ['articleID','customerID'] , how = 'left')
    test = test.merge(l,on = ['articleID','customerID'] , how = 'left')
    
    
    
    """
    l = train[['articleID','customerID','voucherID']].append(test[['articleID','customerID','voucherID']])
    l  = l[['articleID','customerID','voucherID']].groupby(['articleID','customerID']).agg('count').reset_index()
    l.columns = ['articleID','customerID','v_c_a']
    train = train.merge(l,on = ['articleID','customerID'] , how = 'left')
    test = test.merge(l,on = ['articleID','customerID'] , how = 'left')
    """
    
    
    #690073
    l = train[['customerID','voucherID']].append(test[['customerID','voucherID']])
    l  = l[['customerID','voucherID']].groupby(['customerID']).agg('count').reset_index()
    l.columns = ['customerID','v_c_a1']
    train = train.merge(l,on = ['customerID'] , how = 'left')
    test = test.merge(l,on = ['customerID'] , how = 'left')
  
   
    l = train[['articleID','customerID','voucherAmount']].append(test[['articleID','customerID','voucherAmount']])
    l  = l[['articleID','customerID','voucherAmount']].groupby(['articleID','customerID']).agg('mean').reset_index()
    l.columns = ['articleID','customerID','amoun_mean']
    train = train.merge(l,on = ['articleID','customerID'] , how = 'left')
    test = test.merge(l,on = ['articleID','customerID'] , how = 'left')
    
    
    l = train[['articleID','customerID','voucherAmount']].append(test[['articleID','customerID','voucherAmount']])
    l  = l[['articleID','customerID','voucherAmount']].groupby(['articleID','customerID']).agg('std').reset_index()
    l.columns = ['articleID','customerID','amoun_median']
    train = train.merge(l,on = ['articleID','customerID'] , how = 'left')
    test = test.merge(l,on = ['articleID','customerID'] , how = 'left')
   
   
    l = train[['articleID','price']].append(test[['articleID','price']])
    l  = l[['articleID','price']].groupby(['articleID']).agg('mean').reset_index()
    l.columns = ['articleID','price_median_articl']
    train = train.merge(l,on = ['articleID'] , how = 'left')
    test = test.merge(l,on = ['articleID'] , how = 'left')
    
    l = train[['customerID','price']].append(test[['customerID','price']])
    l  = l[['customerID','price']].groupby(['customerID']).agg('mean').reset_index()
    l.columns = ['customerID','price_mean_customerID']
    train = train.merge(l,on = ['customerID'] , how = 'left')
    test = test.merge(l,on = ['customerID'] , how = 'left')
    
    
    l = train[['customerID','price']].append(test[['customerID','price']])
    l  = l[['customerID','price']].groupby(['customerID']).agg('median').reset_index()
    l.columns = ['customerID','price_median_customerID']
    train = train.merge(l,on = ['customerID'] , how = 'left')
    test = test.merge(l,on = ['customerID'] , how = 'left')
    
    
    
    l = train[['customerID','rrp']].append(test[['customerID','rrp']])
    l  = l[['customerID','rrp']].groupby(['customerID']).agg('mean').reset_index()
    l.columns = ['customerID','rrp_mean_customerID']
    train = train.merge(l,on = ['customerID'] , how = 'left')
    test = test.merge(l,on = ['customerID'] , how = 'left')
    
    
    # ek order m kitne ka saamaan magaya,rrp bhi
    l = train[['orderID','price']].append(test[['orderID','price']])
    l  = l[['orderID','price']].groupby(['orderID']).agg('sum').reset_index()
    l.columns = ['orderID','price_sum_orderID']
    train = train.merge(l,on = ['orderID'] , how = 'left')
    test = test.merge(l,on = ['orderID'] , how = 'left')
    
    
    l = train[['orderID','rrp']].append(test[['orderID','rrp']])
    l  = l[['orderID','rrp']].groupby(['orderID']).agg('sum').reset_index()
    l.columns = ['orderID','rrp_sum_orderID']
    train = train.merge(l,on = ['orderID'] , how = 'left')
    test = test.merge(l,on = ['orderID'] , how = 'left')
    
    
   
    """
    yaha se start karna h 
    l = train[['orderID','productGroup']].append(test[['orderID','productGroup']])
    l  = l[['orderID','productGroup']].groupby(['orderID']).agg('count').reset_index()
    l.columns = ['orderID','productGroup_sum_orderID']
    train = train.merge(l,on = ['orderID'] , how = 'left')
    test = test.merge(l,on = ['orderID'] , how = 'left')
    """


    train['orderDate'] =  pd.to_datetime(train['orderDate'], format= '%Y-%m-%d')
    test['orderDate'] =  pd.to_datetime(test['orderDate'], format= '%Y-%m-%d')

    train['year'] = train.orderDate.dt.year
    train['month'] = train.orderDate.dt.month
    train['dayofyear'] = train.orderDate.dt.dayofyear
    train['dayofweek'] = train.orderDate.dt.dayofweek
    train['day'] = train.orderDate.dt.day
    
    test['year'] = test.orderDate.dt.year
    test['month'] = test.orderDate.dt.month
    test['dayofyear'] = test.orderDate.dt.dayofyear
    test['dayofweek'] = test.orderDate.dt.dayofweek
    test['day'] = test.orderDate.dt.day
    
    
    # customer her month kitna kharid raha h 
    l = train[['customerID','month','price']].append(test[['customerID','month','price']])
    l  = l[['customerID','month','price']].groupby(['customerID','month']).agg('median').reset_index()
    l.columns = ['customerID','month','price_median_customerID']
    train = train.merge(l,on = ['customerID','month'] , how = 'left')
    test = test.merge(l,on = ['customerID','month'] , how = 'left')
    
    l = train[['customerID','month','rrp']].append(test[['customerID','month','rrp']])
    l  = l[['customerID','month','rrp']].groupby(['customerID','month']).agg('count').reset_index()
    l.columns = ['customerID','month','rrp_count_customerID']
    train = train.merge(l,on = ['customerID','month'] , how = 'left')
    test = test.merge(l,on = ['customerID','month'] , how = 'left')
    
   
    l = train[['paymentMethod','month','quantity']].append(test[['paymentMethod','month','quantity']])
    l  = l[['paymentMethod','month','quantity']].groupby(['paymentMethod','month']).agg('count').reset_index()
    l.columns = ['paymentMethod','month','quantity_articleID']
    train = train.merge(l,on = ['paymentMethod','month'] , how = 'left')
    test = test.merge(l,on = ['paymentMethod','month'] , how = 'left')
    
    
    l = train[['paymentMethod','month','quantity']].append(test[['paymentMethod','month','quantity']])
    l  = l[['paymentMethod','month','quantity']].groupby(['paymentMethod','month']).agg('sum').reset_index()
    l.columns = ['paymentMethod','month','quantity_articleID']
    train = train.merge(l,on = ['paymentMethod','month'] , how = 'left')
    test = test.merge(l,on = ['paymentMethod','month'] , how = 'left')
    
    
    
    
    l = train[['month','price']].append(test[['month','price']])
    l  = l[['month','price']].groupby(['month']).agg('mean').reset_index()
    l.columns = ['month','y_m_std_price']
    train = train.merge(l,on = ['month'] , how = 'left')
    test = test.merge(l,on = ['month'] , how = 'left')
    
    
    l = train[['customerID','deviceID','price']].append(test[['customerID','deviceID','price']])
    l  = l[['customerID','deviceID','price']].groupby(['customerID','deviceID']).agg('count').reset_index()
    l.columns = ['customerID','deviceID','deviceID_price']
    train = train.merge(l,on = ['customerID','deviceID'] , how = 'left')
    test = test.merge(l,on = ['customerID','deviceID'] , how = 'left')    
  
    
    
    l = train[['deviceID','price']].append(test[['deviceID','price']])
    l  = l[['deviceID','price']].groupby(['deviceID']).agg('mean').reset_index()
    l.columns = ['deviceID','deviceID_price']
    train = train.merge(l,on = ['deviceID'] , how = 'left')
    test = test.merge(l,on = ['deviceID'] , how = 'left')
    
    
    
    
    """
    l = train[['paymentMethod','articleID','month','quantity']].append(test[['paymentMethod','articleID','month','quantity']])
    l  = l[['paymentMethod','articleID','month','quantity']].groupby(['paymentMethod','month','articleID']).agg('count').reset_index()
    l.columns = ['paymentMethod','articleID','month','quantity_articleIDarticleID']
    train = train.merge(l,on = ['paymentMethod','articleID','month'] , how = 'left')
    test = test.merge(l,on = ['paymentMethod','articleID','month'] , how = 'left')
    """
    
   
    
    
    text_columns = []
    more_f =  []
    for f in train.columns:
        if train[f].dtype=='object':
            more_f.append(f)
            text_columns.append(f)            
            lbl = preprocessing.LabelEncoder()
            lbl.fit(list(train[f].values) + list(test[f].values))
            train[f] = lbl.transform(list(train[f].values))
            test[f] = lbl.transform(list(test[f].values)) 
            
    
    
    l = train['orderID'].append(test['orderID'])
    l = l.reset_index()
    l = l.orderID.value_counts().reset_index()
    l.columns = ['orderID','count_order'] 
    train = train.merge(l, on = 'orderID' , how = 'left')
    test = test.merge(l, on = 'orderID' , how = 'left')
    
    
    l = train['customerID'].append(test['customerID'])
    l = l.reset_index()
    l = l.customerID.value_counts().reset_index()
    l.columns = ['customerID','count_cus'] 
    train = train.merge(l, on = 'customerID' , how = 'left')
    test = test.merge(l, on = 'customerID' , how = 'left')
    
    l = train['articleID'].append(test['articleID'])
    l = l.reset_index()
    l = l.articleID.value_counts().reset_index()
    l.columns = ['articleID','count_arti'] 
    train = train.merge(l, on = 'articleID' , how = 'left')
    test = test.merge(l, on = 'articleID' , how = 'left')
    
    
    
    l = train['sizeCode'].append(test['sizeCode'])
    l = l.reset_index()
    l = l.sizeCode.value_counts().reset_index()
    l.columns = ['sizeCode','count_sizeCode'] 
    train = train.merge(l, on = 'sizeCode' , how = 'left')
    test = test.merge(l, on = 'sizeCode' , how = 'left')
    

    # kitne different option h customer k paas
    l = train[['sizeCode','articleID','colorCode']].append(test[['sizeCode','articleID','colorCode']])
    #l = l[['articleID','colorCode']].groupby(['articleID']).colorCode.nunique().reset_index()
    l = l[['sizeCode','articleID','colorCode']].groupby(['articleID','colorCode']).agg('count').reset_index()
    l.columns = ['articleID','colorCode','articleID_grouped_count_with_color_code'] 
    train = train.merge(l, on = ['articleID','colorCode'] , how = 'left')
    test = test.merge(l, on = ['articleID','colorCode'] , how = 'left')
    
    
    

    
    
    
    """
    # color and size for each artical
    l = train[['voucherID','sizeCode','articleID','colorCode']].append(test[['voucherID','sizeCode','articleID','colorCode']])
    l = l.groupby(['articleID','sizeCode','colorCode']).agg('count').reset_index()
    l.columns = ['articleID','sizeCode','colorCode','articleID_grouped_count_with_sizeCode_color'] 
    train = train.merge(l, on = ['articleID','sizeCode','colorCode'] , how = 'left')
    test = test.merge(l, on = ['articleID','sizeCode','colorCode'] , how = 'left')
    
    
    
    
    l = train[['articleID','colorCode']].append(test[['articleID','colorCode']])
    l = l[['articleID','colorCode']].groupby(['articleID']).colorCode.nunique().reset_index()
    
    #l = l[['sizeCode','articleID','colorCode']].groupby(['articleID','colorCode']).agg('count').reset_index()
    l.columns = ['articleID','unique_articleID_grouped_count_with_color_code'] 
    train = train.merge(l, on = 'articleID' , how = 'left')
    test = test.merge(l, on = 'articleID' , how = 'left')    
    """
    
    
    
    
    #m
    #train[['articleID','colorCode']].groupby(['articleID']).agg('count')
    
    """
    Ye kar liya h mene 
    #train['purchase_count'] = train['count_x'] * 1.0/train['count_x']
    #test['purchase_count'] = test['count_x'] * 1.0/test['count_x']
    
    """
    train = train.drop(['orderID','orderDate','sizeCode'],axis =1)
    test = test.drop(['orderID','orderDate','sizeCode'],axis =1)
    
    """
    X = train.loc[train.index <= 1955978]
    Val = train.loc[train.index > 1955978]
    
    
    
    
    
    X_op = X.returnQuantity.values
    Validation_op = Val.returnQuantity.values
    
    X = X.drop('returnQuantity', axis =1)
    127465 0
127852 1
    Val = Val.drop('returnQuantity', axis =1)
    """
    train.replace(np.nan , -1 ,inplace = True)
    test.replace(np.nan , -1 ,inplace = True)
    
    #X.replace(np.nan , -1 ,inplace = True)
    #Val.replace(np.nan , -1 ,inplace = True)
    

    
    

    y  = train.returnQuantity.values
    train = train.drop('returnQuantity', axis =1)
    n_folds = 5
    
    X, y, X_submission = train,y,test
    #test_op = Validation_op
    skf = list(StratifiedKFold(train.month, n_folds))
    
    """
    clfs = [ensemble.RandomForestClassifier(random_state = 42,  min_samples_split = 6 , max_features = 5 ,  n_estimators = 120 , max_depth =  15, n_jobs = -1),
            ensemble.RandomForestClassifier(random_state = 42,  min_samples_split = 6 , max_features = 5 ,  n_estimators = 120 , max_depth =  25, n_jobs = -1),
            ensemble.ExtraTreesClassifier(n_estimators=300,max_depth=10,max_features= 'sqrt',min_samples_split= 35,n_jobs=-1,random_state = 42),
            ensemble.GradientBoostingClassifier(random_state = 42, learning_rate = 0.11,  min_samples_split = 6 , max_features = 5 ,  n_estimators = 120 , max_depth =  5),
            ensemble.GradientBoostingClassifier(random_state = 42, learning_rate = 0.11,  min_samples_split = 6 , max_features = 5 ,  n_estimators = 120 , max_depth =  7),
            ensemble.GradientBoostingClassifier(random_state = 42, learning_rate = 0.11,  min_samples_split = 6 , max_features = 5 ,  n_estimators = 120 , max_depth =  3)
            ]
    """
    
    #678837
    clfs = [ensemble.RandomForestClassifier(random_state = 42,  min_samples_split = 6 , max_features = 5 ,  n_estimators = 120 , max_depth =  15, n_jobs = -1),
            ensemble.RandomForestClassifier(random_state = 42,  min_samples_split = 6 , max_features = 5 ,  n_estimators = 120 , max_depth =  25, n_jobs = -1),
            ensemble.ExtraTreesClassifier(n_estimators=300,max_depth=10,max_features= 'sqrt',min_samples_split= 35,n_jobs=-1,random_state = 42),
            ensemble.ExtraTreesClassifier(n_estimators=300,max_depth=15,max_features= 'sqrt',min_samples_split= 35,n_jobs=-1,random_state = 42),
            ensemble.GradientBoostingClassifier(random_state = 42, learning_rate = 0.11,  min_samples_split = 6 , max_features = 5 ,  n_estimators = 120 , max_depth =  5),
            ensemble.GradientBoostingClassifier(random_state = 42, learning_rate = 0.11,  min_samples_split = 6 , max_features = 5 ,  n_estimators = 120 , max_depth =  7),
            ensemble.GradientBoostingClassifier(random_state = 42, learning_rate = 0.11,  min_samples_split = 6 , max_features = 5 ,  n_estimators = 120 , max_depth =  3)
            ]
    
    
    
    
    print "Creating train and test sets for blending."
    
    dataset_blend_train = np.zeros((X.shape[0], len(clfs)))
    dataset_blend_test = np.zeros((X_submission.shape[0], len(clfs)))
    
    dataset_blend_train1 = np.zeros((X.shape[0], len(clfs)))
    dataset_blend_test1 = np.zeros((X_submission.shape[0], len(clfs)))
    
    for j, clf in enumerate(clfs):
        
        print j, clf
        dataset_blend_test_j = np.zeros((X_submission.shape[0], len(skf)))
        dataset_blend_test_j1 = np.zeros((X_submission.shape[0], len(skf)))
        for i, (train1, test1) in enumerate(skf):
            print "Fold", i
            X_train = X.iloc[train1]
            y_train = y[train1]
            X_test = X.iloc[test1]
            y_test = y[test1]
            clf.fit(X_train, y_train)
            y_submission = clf.predict_proba(X_test)[:,0]
            y_submission1 = clf.predict_proba(X_test)[:,1]
            dataset_blend_train[test1, j] = y_submission
            dataset_blend_train1[test1, j] = y_submission1
            dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:,0]
            dataset_blend_test_j1[:, i] = clf.predict_proba(X_submission)[:,1]
        dataset_blend_test[:,j] = dataset_blend_test_j.mean(1)
        dataset_blend_test1[:,j] = dataset_blend_test_j1.mean(1)
    
    
    
    
    
    
    
    
    print
    print "Blending."
    clf = LogisticRegression(penalty = 'l1', max_iter = 100 )
    clf.fit(dataset_blend_train, y)
    y_submission = clf.predict(dataset_blend_test)

    #score = cal_error(Validation_op,np.array(y_submission))
   # print score,'0'
    
    
    clf = LogisticRegression(penalty = 'l1', max_iter = 100 )
    clf.fit(dataset_blend_train1, y)
    y_submission = clf.predict(dataset_blend_test1)

    #score = cal_error(Validation_op,np.array(y_submission))
    #print score,'1'
    
    
    #678555
    train_total = np.hstack((dataset_blend_train,dataset_blend_train1))
    test_total = np.hstack((dataset_blend_test,dataset_blend_test1))
    
    
    clf = LogisticRegression(penalty = 'l1', max_iter = 100 )
    clf.fit(train_total, y)
    submission = clf.predict(test_total)
    submission_frame['prediction'] = submission
    submission_frame.to_csv('../input/Inst_IT_Allahabad_1.txt', index=None, sep=';')
   
    
